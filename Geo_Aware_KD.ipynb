{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxSUi93x9OMxYKTDQQGPJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PETEROA/Knowledge_Distillation/blob/main/Geo_Aware_KD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GEOMETRY-AWARE KNOWLEDGE DISTILLATION WITH OPTIMAL TRANSPORT:\n",
        "Mathematical Framework:\n",
        "- Optimal Transport (Sinkhorn Divergence)\n",
        "- Riemannian Geometry (Fisher-Rao Metric)\n",
        "- Information Theory (Adaptive Temperature)\n",
        "- Multi-Scale Attention Transfer\n"
      ],
      "metadata": {
        "id": "WsCB9eGF-RT9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jlr4FOQT8l6Y",
        "outputId": "0eef81ac-3605-4313-bcdf-61aec29b6081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 19 11:42:06 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0             27W /   70W |     908MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from typing import Tuple, List, Dict\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 2: OPTIMAL TRANSPORT LOSS (SINKHORN DIVERGENCE)\n",
        "\n",
        "\n",
        "class SinkhornDistance(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, epsilon=0.1, max_iter=100, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.max_iter = max_iter\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def _cost_matrix(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute pairwise squared Euclidean distance matrix.\"\"\"\n",
        "        x_norm = (x ** 2).sum(1).view(-1, 1)\n",
        "        y_norm = (y ** 2).sum(1).view(1, -1)\n",
        "        dist = x_norm + y_norm - 2.0 * torch.mm(x, y.transpose(0, 1))\n",
        "        return dist\n",
        "\n",
        "    def forward(self, source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute Sinkhorn divergence between source and target.\"\"\"\n",
        "        # Flatten spatial dimensions if needed\n",
        "        if source.dim() > 2:\n",
        "            source = source.view(source.size(0), -1)\n",
        "            target = target.view(target.size(0), -1)\n",
        "\n",
        "        # Compute cost matrix\n",
        "        C = self._cost_matrix(source, target)\n",
        "        batch_size = source.size(0)\n",
        "\n",
        "        # Initialize dual variables\n",
        "        mu = torch.ones(batch_size, device=source.device) / batch_size\n",
        "        nu = torch.ones(batch_size, device=target.device) / batch_size\n",
        "\n",
        "        # Sinkhorn iterations\n",
        "        K = torch.exp(-C / self.epsilon)\n",
        "        u = torch.ones_like(mu)\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            v = nu / (K.t() @ u + 1e-8)\n",
        "            u = mu / (K @ v + 1e-8)\n",
        "\n",
        "        # Compute transport plan and cost\n",
        "        pi = u.unsqueeze(1) * K * v.unsqueeze(0)\n",
        "        cost = torch.sum(pi * C)\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            cost = cost / batch_size\n",
        "\n",
        "        return cost\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 3: RIEMANNIAN MANIFOLD REGULARIZATION (FISHER-RAO METRIC)\n",
        "\n",
        "\n",
        "class RiemannianManifoldRegularizer(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def _to_probability(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Convert features to probability distribution via softmax.\"\"\"\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), x.size(1), -1)\n",
        "            x = F.softmax(x, dim=-1)\n",
        "        else:\n",
        "            x = F.softmax(x, dim=-1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, teacher_feat: torch.Tensor, student_feat: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute Fisher-Rao distance between teacher and student features.\"\"\"\n",
        "        # Convert to probability distributions\n",
        "        p = self._to_probability(teacher_feat)\n",
        "        q = self._to_probability(student_feat)\n",
        "\n",
        "        # Flatten for computation\n",
        "        p = p.view(p.size(0), -1)\n",
        "        q = q.view(q.size(0), -1)\n",
        "\n",
        "        # Bhattacharyya coefficient: BC = ∑√(p_i·q_i)\n",
        "        bc = torch.sum(torch.sqrt(p * q + self.epsilon), dim=1)\n",
        "\n",
        "        # Fisher-Rao distance: d_FR = 2·arccos(BC)\n",
        "        bc = torch.clamp(bc, -1.0 + self.epsilon, 1.0 - self.epsilon)\n",
        "        fisher_rao = 2.0 * torch.acos(bc)\n",
        "\n",
        "        return fisher_rao.mean()\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 4: MULTI-SCALE ATTENTION TRANSFER WITH ORTHOGONALITY\n",
        "\n",
        "class MultiScaleAttentionTransfer(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, ortho_weight=0.1):\n",
        "        super().__init__()\n",
        "        self.ortho_weight = ortho_weight\n",
        "\n",
        "    def _compute_attention(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute spatial attention map from features.\"\"\"\n",
        "        # Channel-wise L2 norm\n",
        "        attention = torch.sum(features ** 2, dim=1, keepdim=True)\n",
        "\n",
        "        # Normalize to attention map\n",
        "        b, _, h, w = attention.shape\n",
        "        attention = attention.view(b, -1)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = attention.view(b, 1, h, w)\n",
        "\n",
        "        return attention\n",
        "\n",
        "    def _orthogonality_loss(self, attention: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Enforce orthogonality: ||A^T·A - I||_F²\"\"\"\n",
        "        b, _, h, w = attention.shape\n",
        "        attention_flat = attention.view(b, h * w)\n",
        "\n",
        "        # Compute Gram matrix A^T·A\n",
        "        gram = torch.matmul(attention_flat.t(), attention_flat)\n",
        "\n",
        "        # Identity matrix\n",
        "        identity = torch.eye(h * w, device=attention.device)\n",
        "\n",
        "        # Frobenius norm of difference\n",
        "        ortho_loss = torch.norm(gram - identity, p='fro') ** 2\n",
        "\n",
        "        return ortho_loss / (h * w)\n",
        "\n",
        "    def forward(self, teacher_feat: torch.Tensor, student_feat: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute attention transfer loss with orthogonality constraint.\"\"\"\n",
        "        # Compute attention maps\n",
        "        teacher_att = self._compute_attention(teacher_feat)\n",
        "        student_att = self._compute_attention(student_feat)\n",
        "\n",
        "        # Attention transfer loss\n",
        "        att_loss = torch.norm(teacher_att - student_att, p='fro') ** 2\n",
        "\n",
        "        # Orthogonality regularization\n",
        "        ortho_loss = self._orthogonality_loss(student_att)\n",
        "\n",
        "        total_loss = att_loss + self.ortho_weight * ortho_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 5: ADAPTIVE TEMPERATURE SCHEDULING\n",
        "\n",
        "\n",
        "class AdaptiveTemperatureScheduler(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, T_init=4.0, alpha=0.5, T_min=1.0, T_max=10.0):\n",
        "        super().__init__()\n",
        "        self.T_init = T_init\n",
        "        self.alpha = alpha\n",
        "        self.T_min = T_min\n",
        "        self.T_max = T_max\n",
        "\n",
        "    def _compute_entropy(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute Shannon entropy of prediction distribution.\"\"\"\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        entropy = -torch.sum(probs * log_probs, dim=1)\n",
        "        return entropy.mean()\n",
        "\n",
        "    def forward(self, teacher_logits: torch.Tensor) -> float:\n",
        "        \"\"\"Compute adaptive temperature based on teacher entropy.\"\"\"\n",
        "        entropy = self._compute_entropy(teacher_logits)\n",
        "\n",
        "        # Temperature: T = T_0 · exp(-α · H)\n",
        "        temperature = self.T_init * torch.exp(-self.alpha * entropy)\n",
        "\n",
        "        # Clamp to valid range\n",
        "        temperature = torch.clamp(temperature, self.T_min, self.T_max)\n",
        "\n",
        "        return temperature.item()\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 6: FEATURE ADAPTATION LAYERS\n",
        "\n",
        "\n",
        "class FeatureAdaptation(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, student_channels: List[int], teacher_channels: List[int]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.adaptations = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(s_ch, t_ch, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(t_ch)\n",
        "            )\n",
        "            for s_ch, t_ch in zip(student_channels, teacher_channels)\n",
        "        ])\n",
        "\n",
        "    def forward(self, student_features: List[torch.Tensor]) -> List[torch.Tensor]:\n",
        "        \"\"\"Project student features to teacher dimensions.\"\"\"\n",
        "        adapted_features = [\n",
        "            adapt(feat) for adapt, feat in zip(self.adaptations, student_features)\n",
        "        ]\n",
        "        return adapted_features\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 7: COMPLETE GEOMETRY-AWARE DISTILLATION LOSS\n",
        "\n",
        "\n",
        "class GeometryAwareDistillationLoss(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        lambda_ot=1.0,\n",
        "        lambda_geo=0.5,\n",
        "        lambda_att=2.0,\n",
        "        sinkhorn_eps=0.1,\n",
        "        sinkhorn_iter=100,\n",
        "        ortho_weight=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lambda_ot = lambda_ot\n",
        "        self.lambda_geo = lambda_geo\n",
        "        self.lambda_att = lambda_att\n",
        "\n",
        "        # Initialize components\n",
        "        self.sinkhorn = SinkhornDistance(epsilon=sinkhorn_eps, max_iter=sinkhorn_iter)\n",
        "        self.riemannian = RiemannianManifoldRegularizer()\n",
        "        self.attention = MultiScaleAttentionTransfer(ortho_weight=ortho_weight)\n",
        "        self.temp_scheduler = AdaptiveTemperatureScheduler()\n",
        "\n",
        "    def _kl_divergence_loss(\n",
        "        self,\n",
        "        student_logits: torch.Tensor,\n",
        "        teacher_logits: torch.Tensor,\n",
        "        temperature: float\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Standard KL divergence loss with temperature scaling.\"\"\"\n",
        "        student_log_probs = F.log_softmax(student_logits / temperature, dim=1)\n",
        "        teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
        "\n",
        "        kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
        "\n",
        "        # Scale by T² to maintain gradient magnitude\n",
        "        return kl_loss * (temperature ** 2)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        student_logits: torch.Tensor,\n",
        "        teacher_logits: torch.Tensor,\n",
        "        student_features: List[torch.Tensor],\n",
        "        teacher_features: List[torch.Tensor],\n",
        "        labels: torch.Tensor = None,\n",
        "        alpha: float = 0.7\n",
        "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "\n",
        "        # Adaptive temperature\n",
        "        temperature = self.temp_scheduler(teacher_logits)\n",
        "\n",
        "        # 1. Standard KL divergence\n",
        "        loss_kd = self._kl_divergence_loss(student_logits, teacher_logits, temperature)\n",
        "\n",
        "        # 2. Optimal transport (deepest features)\n",
        "        loss_ot = self.sinkhorn(teacher_features[-1], student_features[-1])\n",
        "\n",
        "        # 3. Riemannian regularization (intermediate features)\n",
        "        loss_geo = sum([\n",
        "            self.riemannian(t_feat, s_feat)\n",
        "            for t_feat, s_feat in zip(teacher_features[:-1], student_features[:-1])\n",
        "        ]) / len(teacher_features[:-1])\n",
        "\n",
        "        # 4. Multi-scale attention transfer\n",
        "        loss_att = sum([\n",
        "            self.attention(t_feat, s_feat)\n",
        "            for t_feat, s_feat in zip(teacher_features, student_features)\n",
        "        ]) / len(teacher_features)\n",
        "\n",
        "        # Combined distillation loss\n",
        "        distillation_loss = (\n",
        "            loss_kd +\n",
        "            self.lambda_ot * loss_ot +\n",
        "            self.lambda_geo * loss_geo +\n",
        "            self.lambda_att * loss_att\n",
        "        )\n",
        "\n",
        "        # Add supervised loss if labels provided\n",
        "        total_loss = distillation_loss\n",
        "        if labels is not None:\n",
        "            loss_ce = F.cross_entropy(student_logits, labels)\n",
        "            total_loss = alpha * distillation_loss + (1 - alpha) * loss_ce\n",
        "\n",
        "        # Loss breakdown\n",
        "        loss_dict = {\n",
        "            'total': total_loss.item(),\n",
        "            'kd': loss_kd.item(),\n",
        "            'ot': loss_ot.item(),\n",
        "            'geo': loss_geo.item(),\n",
        "            'att': loss_att.item(),\n",
        "            'temperature': temperature\n",
        "        }\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_dict['ce'] = loss_ce.item()\n",
        "\n",
        "        return total_loss, loss_dict\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 8: MODEL ARCHITECTURES\n",
        "\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Wrapper for extracting intermediate features using forward hooks.\"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, layer_names: List[str]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.layer_names = layer_names\n",
        "        self.features = {}\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        \"\"\"Register forward hooks to capture intermediate activations.\"\"\"\n",
        "        def get_activation(name):\n",
        "            def hook(module, input, output):\n",
        "                self.features[name] = output\n",
        "            return hook\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if name in self.layer_names:\n",
        "                module.register_forward_hook(get_activation(name))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
        "        \"\"\"Forward pass with feature extraction.\"\"\"\n",
        "        self.features.clear()\n",
        "        logits = self.model(x)\n",
        "        features = [self.features[name] for name in self.layer_names]\n",
        "        return logits, features\n",
        "\n",
        "\n",
        "def create_teacher_model(num_classes=10, pretrained=False):\n",
        "    \"\"\"Create ResNet-50 teacher model with feature extraction.\"\"\"\n",
        "    base_model = models.resnet50(pretrained=pretrained)\n",
        "\n",
        "    # Modify final layer\n",
        "    in_features = base_model.fc.in_features\n",
        "    base_model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    # Feature extraction points\n",
        "    layer_names = ['layer1', 'layer2', 'layer3', 'layer4']\n",
        "\n",
        "    # Channel dimensions for ResNet-50\n",
        "    teacher_channels = [256, 512, 1024, 2048]\n",
        "\n",
        "    model = FeatureExtractor(base_model, layer_names)\n",
        "    return model, layer_names, teacher_channels\n",
        "\n",
        "\n",
        "def create_student_model(num_classes=10, width_mult=0.5):\n",
        "    \"\"\"Create compact ResNet-18 student model.\"\"\"\n",
        "    base_model = models.resnet18(pretrained=False)\n",
        "\n",
        "    # Modify final layer\n",
        "    in_features = base_model.fc.in_features\n",
        "    base_model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    # Same layer names for alignment\n",
        "    layer_names = ['layer1', 'layer2', 'layer3', 'layer4']\n",
        "\n",
        "    # Channel dimensions for ResNet-18\n",
        "    student_channels = [64, 128, 256, 512]\n",
        "\n",
        "    model = FeatureExtractor(base_model, layer_names)\n",
        "    return model, layer_names, student_channels\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    \"\"\"Count trainable parameters.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 9: DATA LOADING\n",
        "\n",
        "\n",
        "def get_cifar10_dataloaders(batch_size=128, num_workers=2, data_dir='./data'):\n",
        "    \"\"\"Create CIFAR-10 train and test dataloaders.\"\"\"\n",
        "\n",
        "    # Training transforms with augmentation\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    # Test transforms\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=train_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=test_transform\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 10: TRAINING FUNCTIONS\n",
        "\n",
        "\n",
        "def train_teacher(\n",
        "    model, train_loader, test_loader,\n",
        "    num_epochs=100, learning_rate=0.1,\n",
        "    device='cuda', save_path='teacher_model.pth'\n",
        "):\n",
        "    \"\"\"Train teacher model from scratch.\"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(), lr=learning_rate,\n",
        "        momentum=0.9, weight_decay=5e-4\n",
        "    )\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
        "    best_acc = 0.0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training Teacher Model ({num_epochs} epochs)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': train_loss / (pbar.n + 1),\n",
        "                'acc': 100. * correct / total\n",
        "            })\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Evaluation\n",
        "        test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f'\\n Best: {test_acc:.2f}%')\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train={train_acc:.2f}%, Test={test_acc:.2f}%')\n",
        "\n",
        "    print(f'\\nTeacher training complete. Best: {best_acc:.2f}%')\n",
        "    return history\n",
        "\n",
        "\n",
        "def distill_student(\n",
        "    teacher, student, train_loader, test_loader,\n",
        "    student_channels, teacher_channels,\n",
        "    num_epochs=200, learning_rate=0.1, alpha=0.7,\n",
        "    device='cuda', save_path='student_model.pth'\n",
        "):\n",
        "    \"\"\"Distill student using geometry-aware loss.\"\"\"\n",
        "\n",
        "    teacher = teacher.to(device)\n",
        "    student = student.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    # Create feature adaptation layers\n",
        "    feature_adapt = FeatureAdaptation(student_channels, teacher_channels).to(device)\n",
        "\n",
        "    # Optimizer for both student and adaptation layers\n",
        "    optimizer = optim.SGD(\n",
        "        list(student.parameters()) + list(feature_adapt.parameters()),\n",
        "        lr=learning_rate, momentum=0.9, weight_decay=5e-4\n",
        "    )\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    distill_criterion = GeometryAwareDistillationLoss(\n",
        "        lambda_ot=1.0, lambda_geo=0.5, lambda_att=2.0\n",
        "    ).to(device)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [],\n",
        "        'loss_components': {'kd': [], 'ot': [], 'geo': [], 'att': [], 'temperature': []}\n",
        "    }\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Distilling Student Model ({num_epochs} epochs)\")\n",
        "    print(f\"Distillation weight α={alpha:.2f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        student.train()\n",
        "        feature_adapt.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        epoch_components = {k: [] for k in ['kd', 'ot', 'geo', 'att', 'temperature']}\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits, teacher_features = teacher(inputs)\n",
        "\n",
        "            student_logits, student_features = student(inputs)\n",
        "\n",
        "            # Adapt student features to match teacher dimensions\n",
        "            adapted_features = feature_adapt(student_features)\n",
        "\n",
        "            loss, loss_dict = distill_criterion(\n",
        "                student_logits, teacher_logits,\n",
        "                adapted_features, teacher_features,\n",
        "                labels, alpha=alpha\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = student_logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            for key in epoch_components:\n",
        "                if key in loss_dict:\n",
        "                    epoch_components[key].append(loss_dict[key])\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': train_loss / (pbar.n + 1),\n",
        "                'acc': 100. * correct / total,\n",
        "                'T': loss_dict['temperature']\n",
        "            })\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        for key in epoch_components:\n",
        "            history['loss_components'][key].append(np.mean(epoch_components[key]))\n",
        "\n",
        "        test_loss, test_acc = evaluate_model(\n",
        "            student, test_loader, nn.CrossEntropyLoss(), device\n",
        "        )\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(student.state_dict(), save_path)\n",
        "            print(f'\\nBest: {test_acc:.2f}%')\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train={train_acc:.2f}%, Test={test_acc:.2f}%')\n",
        "\n",
        "    print(f'\\nStudent distillation complete. Best: {best_acc:.2f}%')\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device='cuda'):\n",
        "    \"\"\"Evaluate model on test set.\"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "    test_acc = 100. * correct / total\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 11: VISUALIZATION\n",
        "\n",
        "\n",
        "def plot_training_history(history, save_path='training_history.png'):\n",
        "    \"\"\"Visualize training history.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss\n",
        "    ax = axes[0, 0]\n",
        "    ax.plot(history['train_loss'], label='Train', linewidth=2)\n",
        "    ax.plot(history['test_loss'], label='Test', linewidth=2)\n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Loss', fontsize=12)\n",
        "    ax.set_title('Loss vs. Epoch', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(history['train_acc'], label='Train', linewidth=2)\n",
        "    ax.plot(history['test_acc'], label='Test', linewidth=2)\n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax.set_title('Accuracy vs. Epoch', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss components\n",
        "    if 'loss_components' in history:\n",
        "        ax = axes[1, 0]\n",
        "        components = history['loss_components']\n",
        "        for key in ['kd', 'ot', 'geo', 'att']:\n",
        "            if key in components and components[key]:\n",
        "                ax.plot(components[key], label=key.upper(), linewidth=2)\n",
        "        ax.set_xlabel('Epoch', fontsize=12)\n",
        "        ax.set_ylabel('Loss Value', fontsize=12)\n",
        "        ax.set_title('Loss Components', fontsize=14, fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Temperature\n",
        "        ax = axes[1, 1]\n",
        "        if 'temperature' in components and components['temperature']:\n",
        "            ax.plot(components['temperature'], linewidth=2, color='red')\n",
        "            ax.set_xlabel('Epoch', fontsize=12)\n",
        "            ax.set_ylabel('Temperature', fontsize=12)\n",
        "            ax.set_title('Adaptive Temperature', fontsize=14, fontweight='bold')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n✓ Plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 12: QUICK DEMO & ANALYSIS\n",
        "\n",
        "\n",
        "def quick_demo():\n",
        "    \"\"\"Run quick demonstration of all components.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GEOMETRY-AWARE KNOWLEDGE DISTILLATION - QUICK DEMO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Test data\n",
        "    batch_size = 16\n",
        "    num_classes = 10\n",
        "\n",
        "    teacher_logits = torch.randn(batch_size, num_classes)\n",
        "    student_logits = torch.randn(batch_size, num_classes)\n",
        "\n",
        "    teacher_features = [\n",
        "        torch.randn(batch_size, 64, 56, 56),\n",
        "        torch.randn(batch_size, 128, 28, 28),\n",
        "        torch.randn(batch_size, 256, 14, 14),\n",
        "        torch.randn(batch_size, 512, 7, 7)\n",
        "    ]\n",
        "\n",
        "    student_features = [\n",
        "        torch.randn(batch_size, 64, 56, 56),\n",
        "        torch.randn(batch_size, 128, 28, 28),\n",
        "        torch.randn(batch_size, 256, 14, 14),\n",
        "        torch.randn(batch_size, 512, 7, 7)\n",
        "    ]\n",
        "\n",
        "    labels = torch.randint(0, num_classes, (batch_size,))\n",
        "\n",
        "    # Initialize loss\n",
        "    distill_loss = GeometryAwareDistillationLoss()\n",
        "\n",
        "    # Compute loss\n",
        "    total_loss, loss_dict = distill_loss(\n",
        "        student_logits, teacher_logits,\n",
        "        student_features, teacher_features,\n",
        "        labels, alpha=0.7\n",
        "    )\n",
        "\n",
        "    print(\"\\n Loss Components:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    for key, value in loss_dict.items():\n",
        "        print(f\"{key:12s}: {value:.6f}\")\n",
        "\n",
        "    print(f\"\\n All components working correctly!\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "\n",
        "def analyze_components():\n",
        "    \"\"\"Detailed analysis of geometric properties.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"MATHEMATICAL ANALYSIS OF COMPONENTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Optimal Transport Analysis\n",
        "    print(\"\\n1️ OPTIMAL TRANSPORT (Sinkhorn Divergence)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    dist1 = torch.randn(100, 2) * 0.5 + torch.tensor([1.0, 1.0])\n",
        "    dist2 = torch.randn(100, 2) * 0.5 + torch.tensor([1.5, 1.5])\n",
        "\n",
        "    sinkhorn = SinkhornDistance(epsilon=0.1, max_iter=100)\n",
        "    ot_dist = sinkhorn(dist1, dist2).item()\n",
        "    l2_dist = torch.mean((dist1 - dist2) ** 2).item()\n",
        "\n",
        "    print(f\"  OT Distance:  {ot_dist:.4f}\")\n",
        "    print(f\"  L2 Distance:  {l2_dist:.4f}\")\n",
        "    print(f\"  Ratio OT/L2:  {ot_dist/l2_dist:.4f}\")\n",
        "    print(\"   OT captures distribution geometry better than L2\")\n",
        "\n",
        "    # 2. Fisher-Rao Analysis\n",
        "    print(\"\\n2️  RIEMANNIAN MANIFOLD (Fisher-Rao Distance)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    p = torch.softmax(torch.randn(1, 1, 10), dim=-1)\n",
        "    q = torch.softmax(torch.randn(1, 1, 10), dim=-1)\n",
        "\n",
        "    riemannian = RiemannianManifoldRegularizer()\n",
        "    fr_dist = riemannian(p, q).item()\n",
        "    eucl_dist = torch.norm(p - q).item()\n",
        "\n",
        "    print(f\"  Fisher-Rao:   {fr_dist:.4f}\")\n",
        "    print(f\"  Euclidean:    {eucl_dist:.4f}\")\n",
        "    print(\"   Geodesic distance respects probability manifold\")\n",
        "\n",
        "    # 3. Attention Analysis\n",
        "    print(\"\\n3️  ATTENTION TRANSFER (Orthogonality Constraint)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    features = torch.randn(4, 64, 14, 14)\n",
        "    attention_module = MultiScaleAttentionTransfer(ortho_weight=0.1)\n",
        "    att_map = attention_module._compute_attention(features)\n",
        "\n",
        "    print(f\"  Attention shape: {att_map.shape}\")\n",
        "    print(f\"  Sum per sample:  {att_map.sum(dim=[1,2,3]).tolist()}\")\n",
        "    print(\"   Attention maps are valid probability distributions\")\n",
        "\n",
        "    # 4. Temperature Analysis\n",
        "    print(\"\\n4️ ADAPTIVE TEMPERATURE (Information Theory)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    confident = torch.randn(10, 10)\n",
        "    confident[0, 0] += 5.0  # Make first class very confident\n",
        "\n",
        "    uncertain = torch.randn(10, 10) * 2.0  # All classes similar\n",
        "\n",
        "    temp_scheduler = AdaptiveTemperatureScheduler()\n",
        "    T_conf = temp_scheduler(confident)\n",
        "    T_uncer = temp_scheduler(uncertain)\n",
        "\n",
        "    print(f\"  T (confident):  {T_conf:.4f}\")\n",
        "    print(f\"  T (uncertain):  {T_uncer:.4f}\")\n",
        "    print(\"  Temperature adapts to teacher confidence\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Analysis complete!\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 13: MAIN EXECUTION\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GEOMETRY-AWARE KNOWLEDGE DISTILLATION\")\n",
        "    print(\"Complete Implementation in Single Colab Notebook\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Quick demo\n",
        "    quick_demo()\n",
        "\n",
        "    # Mathematical analysis\n",
        "    analyze_components()\n",
        "\n",
        "    # Print model info\n",
        "    print(\"\\nMODEL ARCHITECTURE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    teacher, _, teacher_channels = create_teacher_model(num_classes=10, pretrained=False)\n",
        "    student, _, student_channels = create_student_model(num_classes=10, width_mult=0.5)\n",
        "\n",
        "    teacher_params = count_parameters(teacher.model)\n",
        "    student_params = count_parameters(student.model)\n",
        "\n",
        "    print(f\"\\nTeacher (ResNet-50):\")\n",
        "    print(f\"  Parameters: {teacher_params:,}\")\n",
        "    print(f\"  Size: {teacher_params * 4 / (1024**2):.2f} MB\")\n",
        "    print(f\"  Channels: {teacher_channels}\")\n",
        "\n",
        "    print(f\"\\nStudent (ResNet-18, 0.5× width):\")\n",
        "    print(f\"  Parameters: {student_params:,}\")\n",
        "    print(f\"  Size: {student_params * 4 / (1024**2):.2f} MB\")\n",
        "    print(f\"  Channels: {student_channels}\")\n",
        "\n",
        "    print(f\"\\nCompression:\")\n",
        "    print(f\"  Ratio: {teacher_params/student_params:.2f}×\")\n",
        "    print(f\"  Reduction: {(1 - student_params/teacher_params) * 100:.1f}%\")\n",
        "\n",
        "    print(\"\\n Feature Adaptation:\")\n",
        "    print(f\"  Student features are projected to teacher dimensions\")\n",
        "    print(f\"  Enables direct geometric comparison across architectures\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Setup complete! Ready to train.\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# RUN DEMO\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "#  FULL TRAINING\n",
        "\n",
        "# Load data\n",
        "print(\"Loading CIFAR-10...\")\n",
        "train_loader, test_loader = get_cifar10_dataloaders(batch_size=128, num_workers=2)\n",
        "\n",
        "# Create models\n",
        "teacher, _, teacher_channels = create_teacher_model(num_classes=10)\n",
        "student, _, student_channels = create_student_model(num_classes=10)\n",
        "\n",
        "# Train teacher (shorter for demo)\n",
        "print(\"\\nTraining teacher...\")\n",
        "teacher_history = train_teacher(\n",
        "    teacher, train_loader, test_loader,\n",
        "    num_epochs=100,\n",
        "    learning_rate=0.1,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "plot_training_history(teacher_history, 'teacher_history.png')\n",
        "\n",
        "# Distill student\n",
        "print(\"\\nDistilling student...\")\n",
        "student_history = distill_student(\n",
        "    teacher, student, train_loader, test_loader,\n",
        "    student_channels, teacher_channels,  # Pass channel dimensions\n",
        "    num_epochs=200,\n",
        "    learning_rate=0.1,\n",
        "    alpha=0.7,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "plot_training_history(student_history, 'student_history.png')\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ],
      "metadata": {
        "id": "Sb24CuBcOeT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7857ab61-f5b1-4d00-a3f9-e639c9ce7230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "======================================================================\n",
            "GEOMETRY-AWARE KNOWLEDGE DISTILLATION\n",
            "Complete Implementation in Single Colab Notebook\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "GEOMETRY-AWARE KNOWLEDGE DISTILLATION - QUICK DEMO\n",
            "======================================================================\n",
            "\n",
            " Loss Components:\n",
            "==================================================\n",
            "total       : 39.285347\n",
            "kd          : 0.738198\n",
            "ot          : 0.000000\n",
            "geo         : 0.000000\n",
            "att         : 27.121498\n",
            "temperature : 1.497221\n",
            "ce          : 2.661700\n",
            "\n",
            " All components working correctly!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "MATHEMATICAL ANALYSIS OF COMPONENTS\n",
            "======================================================================\n",
            "\n",
            "1️ OPTIMAL TRANSPORT (Sinkhorn Divergence)\n",
            "--------------------------------------------------\n",
            "  OT Distance:  0.0060\n",
            "  L2 Distance:  0.7975\n",
            "  Ratio OT/L2:  0.0075\n",
            "   OT captures distribution geometry better than L2\n",
            "\n",
            "2️  RIEMANNIAN MANIFOLD (Fisher-Rao Distance)\n",
            "--------------------------------------------------\n",
            "  Fisher-Rao:   0.1619\n",
            "  Euclidean:    0.4864\n",
            "   Geodesic distance respects probability manifold\n",
            "\n",
            "3️  ATTENTION TRANSFER (Orthogonality Constraint)\n",
            "--------------------------------------------------\n",
            "  Attention shape: torch.Size([4, 1, 14, 14])\n",
            "  Sum per sample:  [1.0000001192092896, 1.0, 1.0, 0.9999999403953552]\n",
            "   Attention maps are valid probability distributions\n",
            "\n",
            "4️ ADAPTIVE TEMPERATURE (Information Theory)\n",
            "--------------------------------------------------\n",
            "  T (confident):  1.5974\n",
            "  T (uncertain):  2.0124\n",
            "  Temperature adapts to teacher confidence\n",
            "\n",
            "======================================================================\n",
            "Analysis complete!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "MODEL ARCHITECTURE\n",
            "======================================================================\n",
            "\n",
            "Teacher (ResNet-50):\n",
            "  Parameters: 23,528,522\n",
            "  Size: 89.75 MB\n",
            "  Channels: [256, 512, 1024, 2048]\n",
            "\n",
            "Student (ResNet-18, 0.5× width):\n",
            "  Parameters: 11,181,642\n",
            "  Size: 42.65 MB\n",
            "  Channels: [64, 128, 256, 512]\n",
            "\n",
            "Compression:\n",
            "  Ratio: 2.10×\n",
            "  Reduction: 52.5%\n",
            "\n",
            " Feature Adaptation:\n",
            "  Student features are projected to teacher dimensions\n",
            "  Enables direct geometric comparison across architectures\n",
            "\n",
            "======================================================================\n",
            "Setup complete! Ready to train.\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Loading CIFAR-10...\n",
            "\n",
            "Training teacher...\n",
            "\n",
            "============================================================\n",
            "Training Teacher Model (100 epochs)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100: 100%|██████████| 391/391 [00:36<00:00, 10.86it/s, loss=4.62, acc=14.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Best: 17.95%\n",
            "Epoch 1: Train=14.16%, Test=17.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/100: 100%|██████████| 391/391 [00:37<00:00, 10.51it/s, loss=2.08, acc=22.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Best: 26.94%\n",
            "Epoch 2: Train=22.06%, Test=26.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/100:  24%|██▍       | 95/391 [00:09<00:29, 10.06it/s, loss=1.96, acc=27.1]"
          ]
        }
      ]
    }
  ]
}