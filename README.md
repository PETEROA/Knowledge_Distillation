# Knowledge_Distillation
Geometry-Aware Multi-Level Knowledge Distillation with Optimal Transport addresses the fundamental challenge of neural network compression: how to transfer knowledge from a large, accurate teacher model to a compact student model while preserving performance. Traditional knowledge distillation uses simple KL divergence to match output distributions, but this ignores the rich geometric structure in intermediate feature representations.I explore a mathematical  framework that combines optimal transport theory (Sinkhorn divergence for distribution alignment), Riemannian geometry (Fisher-Rao metric on probability manifolds), information theory (entropy-based adaptive temperature), and multi-scale attention transfer with orthogonality constraints. This approach respects the natural geometry of both feature spaces and probability distributions, leading to more effective knowledge transfer across model capacities.
This implementation compresses a ResNet-50 teacher (25.5M parameters) into a ResNet-18 student with 0.5Ã— width multiplier (2.8M parameters) -  on CIFAR-10. This re. The framework attempts to demonstrate that incorporating mathematical principles from optimal transport and differential geometry into distillation losses provides both theoretical elegance and practical gains, making it suitable for edge deployment scenarios where model size and computational efficiency are critical constraints.
